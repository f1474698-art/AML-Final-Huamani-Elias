{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkKJM0Z0WyQn"
      },
      "source": [
        "# üéì Capstone Project - Advanced Machine Learning\n",
        "## TEC-VIII Programa de Especializaci√≥n en Big Data Analytics aplicada a los Negocios\n",
        "\n",
        "---\n",
        "\n",
        "### üìã Informaci√≥n del Proyecto\n",
        "\n",
        "| Campo | Informaci√≥n |\n",
        "|-------|-------------|\n",
        "| **Nombre del Estudiante** | [Elias Huamani] |\n",
        "| **T√≠tulo del Proyecto** | [Predicci√≥n de fallas en motores el√©ctricos de MT] |\n",
        "| **Fecha de Entrega** | [2026-02-19] |\n",
        "| **Profesor** | [Carlos Mari√±o] |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVSlBpc2WyQo"
      },
      "source": [
        "## üìë √çndice\n",
        "\n",
        "1. [Resumen Ejecutivo](#1-resumen-ejecutivo)\n",
        "2. [Configuraci√≥n del Entorno](#2-configuraci√≥n-del-entorno)\n",
        "3. [Definici√≥n del Problema de Negocio](#3-definici√≥n-del-problema-de-negocio)\n",
        "4. [Carga y Exploraci√≥n de Datos](#4-carga-y-exploraci√≥n-de-datos)\n",
        "5. [Preprocesamiento de Datos](#5-preprocesamiento-de-datos)\n",
        "6. [Dise√±o y Arquitectura del Modelo](#6-dise√±o-y-arquitectura-del-modelo)\n",
        "7. [Entrenamiento del Modelo](#7-entrenamiento-del-modelo)\n",
        "8. [Evaluaci√≥n y M√©tricas](#8-evaluaci√≥n-y-m√©tricas)\n",
        "9. [Interpretaci√≥n de Resultados](#9-interpretaci√≥n-de-resultados)\n",
        "10. [Conclusiones y Recomendaciones de Negocio](#10-conclusiones-y-recomendaciones-de-negocio)\n",
        "11. [Referencias](#11-referencias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-yM3dC5WyQo"
      },
      "source": [
        "---\n",
        "## 1. Resumen Ejecutivo\n",
        "\n",
        "**Instrucciones:** Proporcione un resumen conciso (m√°ximo 300 palabras) que incluya:\n",
        "- Problema de negocio abordado\n",
        "- Metodolog√≠a utilizada\n",
        "- Principales hallazgos\n",
        "- Impacto esperado en el negocio\n",
        "\n",
        "---\n",
        "\n",
        "*[Este trabajo busca desarrollar un sistema de anal√≠tica predictiva basado en ML para anticipar fallas en motores el√©ctricos de media tensi√≥n en una planta minera, utilizando datos de sensores de vibraci√≥n y variables operacionales (carga, RPM, temperatura). El enfoque busca pasar de alarmas por umbrales a una gesti√≥n proactiva del riesgo, entregando un puntaje de probabilidad de falla en un horizonte definido y recomendaciones de acci√≥n para mantenimiento]*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgVuTc6VWyQp"
      },
      "source": [
        "## 2. Configuraci√≥n del Entorno\n",
        "\n",
        "### 2.1 Verificaci√≥n de GPU (Recomendado para Deep Learning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWN-KjyBWyQp",
        "outputId": "e6f92ddb-3af0-4f69-ba08-6d6e3a0fd15e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ GPU disponible: Tesla T4\n",
            "   Memoria GPU: 15.64 GB\n",
            "\n",
            "Dispositivo seleccionado: cuda\n"
          ]
        }
      ],
      "source": [
        "# Verificar si hay GPU disponible\n",
        "import torch\n",
        "\n",
        "# Verificar disponibilidad de GPU\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU disponible: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memoria GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è GPU no disponible. Usando CPU.\")\n",
        "    print(\"   Recomendaci√≥n: En Colab, vaya a Runtime > Change runtime type > GPU\")\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "print(f\"\\nDispositivo seleccionado: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kg80ugpsWyQp"
      },
      "source": [
        "### 2.2 Instalaci√≥n de Librer√≠as Adicionales (si es necesario)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hn7_xmwwWyQp",
        "outputId": "3261d90e-215f-499a-fe7f-26f92893b68f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.21.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.23.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: typer>=0.23.1 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (0.23.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.23.1->typer-slim->transformers) (8.3.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.23.1->typer-slim->transformers) (13.9.4)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.23.1->typer-slim->transformers) (0.0.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer>=0.23.1->typer-slim->transformers) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer>=0.23.1->typer-slim->transformers) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.23.1->typer-slim->transformers) (0.1.2)\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.6.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (2.9.0+cu128)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (4.67.3)\n",
            "Requirement already satisfied: PyYAML>5.4 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (6.0.3)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.3.0)\n",
            "Collecting torchmetrics>0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (26.0)\n",
            "Requirement already satisfied: typing-extensions>4.5.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (4.15.0)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.13.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.21.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.5.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics>0.7.0->pytorch-lightning) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.22.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.3)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.11)\n",
            "Downloading pytorch_lightning-2.6.1-py3-none-any.whl (857 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m857.3/857.3 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.15.2 pytorch-lightning-2.6.1 torchmetrics-1.8.2\n"
          ]
        }
      ],
      "source": [
        "# Descomente e instale las librer√≠as adicionales que necesite\n",
        "!pip install transformers\n",
        "!pip install pytorch-lightning\n",
        "# !pip install optuna\n",
        "# !pip install shap\n",
        "# !pip install lime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVJcm3YCWyQp"
      },
      "source": [
        "### 2.3 Importaci√≥n de Librer√≠as"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNjzyFVeWyQq",
        "outputId": "dc2b6703-b554-4cf3-f1cc-cc128c1aa97a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Todas las librer√≠as importadas correctamente\n",
            "   PyTorch version: 2.9.0+cu128\n",
            "   TensorFlow version: 2.19.0\n"
          ]
        }
      ],
      "source": [
        "# =====================================================\n",
        "# LIBRER√çAS FUNDAMENTALES\n",
        "# =====================================================\n",
        "\n",
        "# Manipulaci√≥n de datos\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Visualizaci√≥n\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Deep Learning - PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "\n",
        "# Deep Learning - TensorFlow/Keras (alternativa)\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "# Preprocesamiento\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, confusion_matrix, classification_report,\n",
        "                             mean_squared_error, mean_absolute_error, r2_score)\n",
        "\n",
        "# Utilidades\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuraci√≥n de visualizaci√≥n\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette('husl')\n",
        "%matplotlib inline\n",
        "\n",
        "# Semilla para reproducibilidad\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "print(\"‚úÖ Todas las librer√≠as importadas correctamente\")\n",
        "print(f\"   PyTorch version: {torch.__version__}\")\n",
        "print(f\"   TensorFlow version: {tf.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRwaNOEbWyQq"
      },
      "source": [
        "### 2.4 Conexi√≥n con Google Drive (para cargar datos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BRGttAgWyQq",
        "outputId": "e3edf1b1-6a34-4c57-a598-7baaf1add7c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "‚úÖ Google Drive montado\n",
            "   Ruta base del proyecto: /content/drive/MyDrive/Capstone_Project/\n"
          ]
        }
      ],
      "source": [
        "# Montar Google Drive para acceder a los datos\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Definir la ruta base de su proyecto\n",
        "# Modifique esta ruta seg√∫n la ubicaci√≥n de sus datos\n",
        "BASE_PATH = '/content/drive/MyDrive/Capstone_Project/'\n",
        "\n",
        "print(f\"‚úÖ Google Drive montado\")\n",
        "print(f\"   Ruta base del proyecto: {BASE_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fDcPV_kWyQq"
      },
      "source": [
        "---\n",
        "## 3. Definici√≥n del Problema de Negocio\n",
        "\n",
        "### 3.1 Contexto del Negocio\n",
        "\n",
        "**Instrucciones:** Describa el contexto empresarial, incluyendo:\n",
        "- Industria/Sector\n",
        "- Empresa o caso de estudio\n",
        "- Situaci√≥n actual\n",
        "\n",
        "---\n",
        "\n",
        "*[El sector donde se desarrolla el presente proyecto es la industria minera debido a la criticidad de los motores de media teni√≥n en una producci√≥n cont√≠nua]*\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 3.2 Problema a Resolver\n",
        "\n",
        "**Instrucciones:** Defina claramente:\n",
        "- ¬øCu√°l es el problema espec√≠fico?\n",
        "- ¬øPor qu√© es importante resolverlo?\n",
        "- ¬øCu√°l es el impacto actual del problema?\n",
        "\n",
        "---\n",
        "\n",
        "*[Los motores el√©ctricos de media tensi√≥n son activos cr√≠ticos en operaciones mineras. La severidad del ambiente y la criticidad del proceso hacen que una falla derive en altos costos por p√©rdida de producci√≥n y da√±o secundario]*\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 3.3 Objetivos del Proyecto\n",
        "\n",
        "**Instrucciones:** Liste los objetivos SMART (Espec√≠ficos, Medibles, Alcanzables, Relevantes, Temporales)\n",
        "\n",
        "---\n",
        "\n",
        "**Objetivo General:**\n",
        "*[Desarrollar e implementar un modelo de ML que estime el riesgo de falla de motores de MT en un horizonte de tiempo usando sensores de vibraci√≥n y variables operacionales para soportar decisones de inspecci√≥n, planificaci√≥n y mantenimiento]*\n",
        "\n",
        "**Objetivos Espec√≠ficos:**\n",
        "1. *[Detectar patrones de degradaci√≥n y diferenciarlos de variabilidad operacional]*\n",
        "2. *[Predecir fallas relevantes dentro del horizonte de tiempo mediante clasificaci√≥n supervisada]*\n",
        "3. *[Entregar explicabilidad por alerta para sustentar la decisi√≥n t√©cnica]*\n",
        "\n",
        "---\n",
        "\n",
        "### 3.4 Tipo de Problema de Machine Learning\n",
        "\n",
        "**Instrucciones:** Identifique el tipo de problema:\n",
        "- [X] Clasificaci√≥n binaria\n",
        "- [ ] Clasificaci√≥n multiclase\n",
        "- [ ] Regresi√≥n\n",
        "- [ ] Clustering\n",
        "- [ ] Series temporales\n",
        "- [ ] Procesamiento de Lenguaje Natural (NLP)\n",
        "- [ ] Visi√≥n por Computadora\n",
        "- [ ] Otro: _________\n",
        "\n",
        "**Justificaci√≥n:**\n",
        "*[Es principalmente un problema de aprendeizaje supervisado de clasificaci√≥n binaria debido a que con las entradas de datos se determinar√° si en un horizonte de tiempo determinado un equipo fallar√° o no (0 o 1)]*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7-AuM3YWyQq"
      },
      "source": [
        "---\n",
        "## 4. Carga y Exploraci√≥n de Datos\n",
        "\n",
        "### 4.1 Carga de Datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zmkoq85WyQq"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# CARGA DE DATOS\n",
        "# =====================================================\n",
        "\n",
        "# Opci√≥n 1: Cargar desde Google Drive\n",
        "# df = pd.read_csv(BASE_PATH + 'datos.csv')\n",
        "# ==========================================================================\n",
        "# LA CARGA DE DATOS SERIA DESDE UN ARCHIVO CSV, DEBIDO A RESTRICCIONES DE LA\n",
        "# EMPRESA NO SE TUVO ACCESO\n",
        "# ==========================================================================\n",
        "# Opci√≥n 2: Cargar desde URL\n",
        "# df = pd.read_csv('https://url-de-sus-datos.com/datos.csv')\n",
        "\n",
        "# Opci√≥n 3: Cargar desde archivo local (subido a Colab)\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# df = pd.read_csv('nombre_archivo.csv')\n",
        "\n",
        "# Opci√≥n 4: Dataset de ejemplo (para testing)\n",
        "# from sklearn.datasets import load_iris, load_boston, fetch_california_housing\n",
        "# data = load_iris()\n",
        "# df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "# df['target'] = data.target\n",
        "\n",
        "# =====================================================\n",
        "# COMPLETE AQU√ç: Cargue su dataset\n",
        "# =====================================================\n",
        "\n",
        "# df = pd.read_csv('...')  # Descomente y complete\n",
        "\n",
        "print(f\"‚úÖ Dataset cargado exitosamente\")\n",
        "print(f\"   Dimensiones: {df.shape[0]:,} filas √ó {df.shape[1]} columnas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TYJbL5VWyQr"
      },
      "source": [
        "### 4.2 Descripci√≥n del Dataset\n",
        "\n",
        "**Instrucciones:** Describa su dataset:\n",
        "- Fuente de los datos\n",
        "- Per√≠odo de tiempo que cubren\n",
        "- Descripci√≥n de cada variable\n",
        "\n",
        "---\n",
        "\n",
        "| Variable | Tipo | Descripci√≥n |\n",
        "|----------|------|-------------|\n",
        "| VE_1 | num√©rica | [Niveles de vibraci√≥n de motores el√©ctricos] |\n",
        "| TE_1 | num√©rica | [Temperatura de rodamientos de motores el√©ctricos] |\n",
        "| Date | num√©rica | [Fecha de la muestra] |\n",
        "| Motor_ID | Categorica | ID de equipo |\n",
        "| Area | Categorica | Ubicaci√≥n de equipo |\n",
        "| Tipo | Categorica | Equipo al que pertenece |\n",
        "| Y_faill_H | num√©rica | [Falla en el equipo en un tiempo determinado] |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQPzSfbeWyQr"
      },
      "source": [
        "### 4.3 Exploraci√≥n Inicial de Datos (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "VZm7avvJWyQr",
        "outputId": "f5fcae11-0e07-46f8-f56b-bbb07569cbcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "INFORMACI√ìN GENERAL DEL DATASET\n",
            "============================================================\n",
            "\n",
            "üìä Primeras 5 filas:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3772419769.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Primeras filas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nüìä Primeras 5 filas:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Informaci√≥n del dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "# =====================================================\n",
        "# INFORMACI√ìN GENERAL DEL DATASET\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"DATA RECOLECTADA DE SENSORES EN CAMPO\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Primeras filas\n",
        "print(\"\\nüìä Primeras 5 filas:\")\n",
        "display(df.head())\n",
        "\n",
        "# Informaci√≥n del dataset\n",
        "print(\"\\nüìã Informaci√≥n del Dataset:\")\n",
        "print(df.info())\n",
        "\n",
        "# Estad√≠sticas descriptivas\n",
        "print(\"\\nüìà Estad√≠sticas Descriptivas:\")\n",
        "display(df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NirdhGaMWyQr"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# AN√ÅLISIS DE VALORES FALTANTES\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"AN√ÅLISIS DE VALORES FALTANTES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Calcular valores faltantes\n",
        "missing_data = pd.DataFrame({\n",
        "    'Total Faltantes': df.isnull().sum(),\n",
        "    'Porcentaje (%)': (df.isnull().sum() / len(df) * 100).round(2)\n",
        "})\n",
        "missing_data = missing_data[missing_data['Total Faltantes'] > 0].sort_values('Porcentaje (%)', ascending=False)\n",
        "\n",
        "if len(missing_data) > 0:\n",
        "    print(\"\\n‚ö†Ô∏è Variables con valores faltantes:\")\n",
        "    display(missing_data)\n",
        "\n",
        "    # Visualizaci√≥n de valores faltantes\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=missing_data.index, y='Porcentaje (%)', data=missing_data)\n",
        "    plt.title('Porcentaje de Valores Faltantes por Variable')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.ylabel('Porcentaje (%)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\n‚úÖ No hay valores faltantes en el dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7hCQOXLWyQr"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# AN√ÅLISIS DE LA VARIABLE OBJETIVO\n",
        "# =====================================================\n",
        "\n",
        "# COMPLETE: Especifique el nombre de su variable objetivo\n",
        "TARGET_COLUMN = 'Y_fail_H'  # Cambie 'target' por el nombre de su variable objetivo\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"AN√ÅLISIS DE LA VARIABLE OBJETIVO: {TARGET_COLUMN}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Para clasificaci√≥n\n",
        "if df[TARGET_COLUMN].dtype == 'object' or df[TARGET_COLUMN].nunique() < 20:\n",
        "    print(\"\\nüìä Distribuci√≥n de clases:\")\n",
        "    class_dist = df[TARGET_COLUMN].value_counts()\n",
        "    print(class_dist)\n",
        "\n",
        "    # Visualizaci√≥n\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Gr√°fico de barras\n",
        "    sns.countplot(data=df, x=TARGET_COLUMN, ax=axes[0])\n",
        "    axes[0].set_title(f'Distribuci√≥n de {TARGET_COLUMN}')\n",
        "    axes[0].set_xlabel(TARGET_COLUMN)\n",
        "    axes[0].set_ylabel('Frecuencia')\n",
        "\n",
        "    # Gr√°fico de pastel\n",
        "    axes[1].pie(class_dist.values, labels=class_dist.index, autopct='%1.1f%%', startangle=90)\n",
        "    axes[1].set_title(f'Proporci√≥n de {TARGET_COLUMN}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Verificar desbalance\n",
        "    imbalance_ratio = class_dist.max() / class_dist.min()\n",
        "    if imbalance_ratio > 3:\n",
        "        print(f\"\\n‚ö†Ô∏è ADVERTENCIA: Dataset desbalanceado (ratio {imbalance_ratio:.2f}:1)\")\n",
        "        print(\"   Considere t√©cnicas de balanceo: SMOTE, undersampling, class weights\")\n",
        "else:\n",
        "    # Para regresi√≥n\n",
        "    print(\"\\nüìä Estad√≠sticas de la variable objetivo:\")\n",
        "    print(df[TARGET_COLUMN].describe())\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Histograma\n",
        "    sns.histplot(df[TARGET_COLUMN], kde=True, ax=axes[0])\n",
        "    axes[0].set_title(f'Distribuci√≥n de {TARGET_COLUMN}')\n",
        "\n",
        "    # Box plot\n",
        "    sns.boxplot(y=df[TARGET_COLUMN], ax=axes[1])\n",
        "    axes[1].set_title(f'Box Plot de {TARGET_COLUMN}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUooy6LhWyQr"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# AN√ÅLISIS DE CORRELACIONES\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"MATRIZ DE CORRELACIONES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Seleccionar solo columnas num√©ricas\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "if len(numeric_cols) > 1:\n",
        "    # Calcular correlaciones\n",
        "    correlation_matrix = df[numeric_cols].corr()\n",
        "\n",
        "    # Visualizaci√≥n\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm',\n",
        "                center=0, fmt='.2f', linewidths=0.5)\n",
        "    plt.title('Matriz de Correlaciones')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Correlaciones con la variable objetivo\n",
        "    if TARGET_COLUMN in numeric_cols:\n",
        "        print(f\"\\nüìä Correlaciones con {TARGET_COLUMN}:\")\n",
        "        target_corr = correlation_matrix[TARGET_COLUMN].drop(TARGET_COLUMN).sort_values(ascending=False)\n",
        "        print(target_corr)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No hay suficientes columnas num√©ricas para an√°lisis de correlaci√≥n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgUKm6SfWyQr"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# VISUALIZACIONES ADICIONALES\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"VISUALIZACIONES ADICIONALES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Distribuci√≥n de variables num√©ricas\n",
        "numeric_cols_plot = df.select_dtypes(include=[np.number]).columns[:8]  # Primeras 8 columnas\n",
        "\n",
        "if len(numeric_cols_plot) > 0:\n",
        "    n_cols = 2\n",
        "    n_rows = (len(numeric_cols_plot) + 1) // 2\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 4*n_rows))\n",
        "    axes = axes.flatten() if n_rows > 1 else [axes]\n",
        "\n",
        "    for i, col in enumerate(numeric_cols_plot):\n",
        "        if i < len(axes):\n",
        "            sns.histplot(df[col], kde=True, ax=axes[i])\n",
        "            axes[i].set_title(f'Distribuci√≥n de {col}')\n",
        "\n",
        "    # Ocultar ejes vac√≠os\n",
        "    for j in range(i+1, len(axes)):\n",
        "        axes[j].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq-kgHEyWyQr"
      },
      "source": [
        "### 4.4 Hallazgos del EDA\n",
        "\n",
        "**Instrucciones:** Resuma los principales hallazgos de la exploraci√≥n de datos:\n",
        "\n",
        "---\n",
        "\n",
        "**Hallazgos Principales:**\n",
        "1. *[Gaps de datos por sensores, periodos sin registro de medici√≥n]*\n",
        "2. *[Sensores deteriorados producen una se√±al plana sin variaci√≥n]*\n",
        "3. *[Diferencia de datos cuando el equipo esta operando o apagado (ON OFF)]*\n",
        "\n",
        "**Problemas Identificados:**\n",
        "1. *[Datos faltantes de los sensores por diversos motivos]*\n",
        "2. *[Mala se√±al de los sensores o ruido estructural que confunde las lecturas]*\n",
        "\n",
        "**Acciones a Tomar:**\n",
        "1. *[Separar las lecturas perdidas (sin datos)para evitar decisiones erroneas]*\n",
        "2. *[Incorporar chequeos autom√°ticos de salud del sensor para determinar su integridad y buen funcionamiento]*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jenLJ7NjWyQr"
      },
      "source": [
        "---\n",
        "## 5. Preprocesamiento de Datos\n",
        "\n",
        "### 5.1 Tratamiento de Valores Faltantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tu7d5mVrWyQr"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# TRATAMIENTO DE VALORES FALTANTES\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"TRATAMIENTO DE VALORES FALTANTES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Crear copia del dataframe\n",
        "df_clean = df.copy()\n",
        "# =================================================================================\n",
        "# OPCI√ìN DE TOMAR EN CUENTA DEBIDO A LA CRITICIDAD DE LAS LECTURAS PARA EL ANALISIS\n",
        "# Opci√≥n 1: Eliminar filas con valores faltantes\n",
        "# df_clean = df_clean.dropna()\n",
        "# =================================================================================\n",
        "\n",
        "# Opci√≥n 2: Imputar con la media (variables num√©ricas)\n",
        "# from sklearn.impute import SimpleImputer\n",
        "# imputer = SimpleImputer(strategy='mean')\n",
        "# df_clean[numeric_cols] = imputer.fit_transform(df_clean[numeric_cols])\n",
        "\n",
        "# Opci√≥n 3: Imputar con la moda (variables categ√≥ricas)\n",
        "# for col in categorical_cols:\n",
        "#     df_clean[col].fillna(df_clean[col].mode()[0], inplace=True)\n",
        "\n",
        "# Opci√≥n 4: Imputaci√≥n avanzada con KNN\n",
        "# from sklearn.impute import KNNImputer\n",
        "# imputer = KNNImputer(n_neighbors=5)\n",
        "# df_clean[numeric_cols] = imputer.fit_transform(df_clean[numeric_cols])\n",
        "\n",
        "# =====================================================\n",
        "# COMPLETE AQU√ç: Aplique su estrategia de imputaci√≥n\n",
        "# =====================================================\n",
        "\n",
        "# ELIMINAR FILAS CON VALORES FALTANTES\n",
        "df_clean = df_clean.dropna()\n",
        "\n",
        "print(f\"\\n‚úÖ Valores faltantes tratados\")\n",
        "print(f\"   Filas restantes: {len(df_clean):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfqdYlcyWyQr"
      },
      "source": [
        "### 5.2 Tratamiento de Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCgUEY6BWyQs"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# DETECCI√ìN Y TRATAMIENTO DE OUTLIERS\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"DETECCI√ìN DE OUTLIERS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def detect_outliers_iqr(data, column):\n",
        "    \"\"\"Detecta outliers usando el m√©todo IQR\"\"\"\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
        "    return len(outliers), lower_bound, upper_bound\n",
        "\n",
        "# Detectar outliers en cada columna num√©rica\n",
        "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "outlier_summary = []\n",
        "for col in numeric_cols:\n",
        "    n_outliers, lower, upper = detect_outliers_iqr(df_clean, col)\n",
        "    if n_outliers > 0:\n",
        "        outlier_summary.append({\n",
        "            'Variable': col,\n",
        "            'N_Outliers': n_outliers,\n",
        "            'Porcentaje (%)': round(n_outliers/len(df_clean)*100, 2),\n",
        "            'L√≠mite_Inferior': round(lower, 2),\n",
        "            'L√≠mite_Superior': round(upper, 2)\n",
        "        })\n",
        "\n",
        "if outlier_summary:\n",
        "    outlier_df = pd.DataFrame(outlier_summary)\n",
        "    print(\"\\n‚ö†Ô∏è Variables con outliers detectados:\")\n",
        "    display(outlier_df)\n",
        "else:\n",
        "    print(\"\\n‚úÖ No se detectaron outliers significativos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiGDEd4yWyQs"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# TRATAMIENTO DE OUTLIERS (OPCIONAL)\n",
        "# =====================================================\n",
        "\n",
        "# Opci√≥n 1: Eliminar outliers\n",
        "# for col in numeric_cols:\n",
        "#     Q1, Q3 = df_clean[col].quantile([0.25, 0.75])\n",
        "#     IQR = Q3 - Q1\n",
        "#     df_clean = df_clean[(df_clean[col] >= Q1 - 1.5*IQR) & (df_clean[col] <= Q3 + 1.5*IQR)]\n",
        "\n",
        "# Opci√≥n 2: Capear outliers (winsorizing)\n",
        "from scipy.stats import mstats\n",
        "for col in numeric_cols:\n",
        "df_clean[col] = mstats.winsorize(df_clean[col], limits=[0.05, 0.05])\n",
        "\n",
        "# Opci√≥n 3: Transformaci√≥n logar√≠tmica\n",
        "# for col in cols_to_transform:\n",
        "#     df_clean[col] = np.log1p(df_clean[col])\n",
        "\n",
        "# =====================================================\n",
        "# COMPLETE AQU√ç: Aplique su estrategia de tratamiento\n",
        "# =====================================================\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjX3eR8bWyQs"
      },
      "source": [
        "### 5.3 Codificaci√≥n de Variables Categ√≥ricas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0yu26aFWyQs"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# CODIFICACI√ìN DE VARIABLES CATEG√ìRICAS\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CODIFICACI√ìN DE VARIABLES CATEG√ìRICAS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Identificar variables categ√≥ricas\n",
        "categorical_cols = df_clean.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "print(f\"\\nVariables categ√≥ricas encontradas: {categorical_cols}\")\n",
        "\n",
        "Opci√≥n 1: Label Encoding (para variables ordinales o target)\n",
        "le = LabelEncoder()\n",
        "df_clean['columna_encoded'] = le.fit_transform(df_clean['columna'])\n",
        "\n",
        "# Opci√≥n 2: One-Hot Encoding (para variables nominales)\n",
        "# df_clean = pd.get_dummies(df_clean, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "# Opci√≥n 3: Target Encoding\n",
        "# from sklearn.preprocessing import TargetEncoder\n",
        "# encoder = TargetEncoder()\n",
        "# df_clean[categorical_cols] = encoder.fit_transform(df_clean[categorical_cols], df_clean[TARGET_COLUMN])\n",
        "\n",
        "# =====================================================\n",
        "# COMPLETE AQU√ç: Aplique su estrategia de codificaci√≥n\n",
        "# =====================================================\n",
        "\n",
        "\n",
        "\n",
        "print(f\"\\n‚úÖ Codificaci√≥n completada\")\n",
        "print(f\"   Dimensiones finales: {df_clean.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTs2WW-3WyQs"
      },
      "source": [
        "### 5.4 Escalado/Normalizaci√≥n de Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWp6cSMyWyQs"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# ESCALADO DE FEATURES\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ESCALADO DE FEATURES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Separar features y target\n",
        "X = df_clean.drop(columns=[TARGET_COLUMN])\n",
        "y = df_clean[TARGET_COLUMN]\n",
        "\n",
        "print(f\"\\nDimensiones de X: {X.shape}\")\n",
        "print(f\"Dimensiones de y: {y.shape}\")\n",
        "\n",
        "# Opci√≥n 1: StandardScaler (media=0, std=1) - Recomendado para redes neuronales\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Opci√≥n 2: MinMaxScaler (rango [0,1])\n",
        "# scaler = MinMaxScaler()\n",
        "\n",
        "# Opci√≥n 3: RobustScaler (robusto a outliers)\n",
        "# from sklearn.preprocessing import RobustScaler\n",
        "# scaler = RobustScaler()\n",
        "\n",
        "# Aplicar escalado\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
        "\n",
        "print(f\"\\n‚úÖ Escalado completado usando {type(scaler).__name__}\")\n",
        "print(f\"   Media de features: {X_scaled.mean().mean():.6f}\")\n",
        "print(f\"   Std de features: {X_scaled.std().mean():.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biocffxEWyQs"
      },
      "source": [
        "### 5.5 Divisi√≥n de Datos (Train/Validation/Test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6h4IdDJeWyQs"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# DIVISI√ìN DE DATOS\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"DIVISI√ìN DE DATOS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Divisi√≥n en train (70%), validation (15%), test (15%)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.15, random_state=RANDOM_SEED, stratify=y if y.dtype == 'object' or y.nunique() < 20 else None\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.176, random_state=RANDOM_SEED, stratify=y_temp if y_temp.dtype == 'object' or y_temp.nunique() < 20 else None  # 0.176 ‚âà 15% del total\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä Divisi√≥n de datos:\")\n",
        "print(f\"   Training set:   {X_train.shape[0]:,} muestras ({X_train.shape[0]/len(X_scaled)*100:.1f}%)\")\n",
        "print(f\"   Validation set: {X_val.shape[0]:,} muestras ({X_val.shape[0]/len(X_scaled)*100:.1f}%)\")\n",
        "print(f\"   Test set:       {X_test.shape[0]:,} muestras ({X_test.shape[0]/len(X_scaled)*100:.1f}%)\")\n",
        "\n",
        "# Verificar distribuci√≥n de clases (para clasificaci√≥n)\n",
        "if y.dtype == 'object' or y.nunique() < 20:\n",
        "    print(f\"\\nüìä Distribuci√≥n de clases en cada conjunto:\")\n",
        "    print(f\"   Train: {dict(y_train.value_counts(normalize=True).round(3))}\")\n",
        "    print(f\"   Val:   {dict(y_val.value_counts(normalize=True).round(3))}\")\n",
        "    print(f\"   Test:  {dict(y_test.value_counts(normalize=True).round(3))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnrSmDZWWyQs"
      },
      "source": [
        "### 5.6 Preparaci√≥n de Datos para Deep Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tawhe8aYWyQs"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# PREPARACI√ìN PARA PYTORCH\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"PREPARACI√ìN DE DATOS PARA PYTORCH\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Convertir a tensores de PyTorch\n",
        "X_train_tensor = torch.FloatTensor(X_train.values)\n",
        "X_val_tensor = torch.FloatTensor(X_val.values)\n",
        "X_test_tensor = torch.FloatTensor(X_test.values)\n",
        "\n",
        "# Para clasificaci√≥n\n",
        "if y.dtype == 'object' or y.nunique() < 20:\n",
        "    # Codificar labels si es necesario\n",
        "    if y_train.dtype == 'object':\n",
        "        label_encoder = LabelEncoder()\n",
        "        y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "        y_val_encoded = label_encoder.transform(y_val)\n",
        "        y_test_encoded = label_encoder.transform(y_test)\n",
        "    else:\n",
        "        y_train_encoded = y_train.values\n",
        "        y_val_encoded = y_val.values\n",
        "        y_test_encoded = y_test.values\n",
        "\n",
        "    y_train_tensor = torch.LongTensor(y_train_encoded)\n",
        "    y_val_tensor = torch.LongTensor(y_val_encoded)\n",
        "    y_test_tensor = torch.LongTensor(y_test_encoded)\n",
        "else:\n",
        "    # Para regresi√≥n\n",
        "    y_train_tensor = torch.FloatTensor(y_train.values).unsqueeze(1)\n",
        "    y_val_tensor = torch.FloatTensor(y_val.values).unsqueeze(1)\n",
        "    y_test_tensor = torch.FloatTensor(y_test.values).unsqueeze(1)\n",
        "\n",
        "# Crear DataLoaders\n",
        "BATCH_SIZE = 32  # Ajuste seg√∫n su dataset\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"\\n‚úÖ DataLoaders creados\")\n",
        "print(f\"   Batch size: {BATCH_SIZE}\")\n",
        "print(f\"   Batches de entrenamiento: {len(train_loader)}\")\n",
        "print(f\"   Batches de validaci√≥n: {len(val_loader)}\")\n",
        "print(f\"   Batches de test: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ne_0W6vWyQs"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# PREPARACI√ìN PARA TENSORFLOW/KERAS (ALTERNATIVA)\n",
        "# =====================================================\n",
        "\n",
        "#print(\"=\" * 60)\n",
        "#print(\"PREPARACI√ìN DE DATOS PARA TENSORFLOW/KERAS\")\n",
        "#print(\"=\" * 60)\n",
        "\n",
        "# Convertir a arrays numpy (Keras acepta DataFrames directamente, pero es mejor convertir)\n",
        "#X_train_np = X_train.values.astype('float32')\n",
        "#X_val_np = X_val.values.astype('float32')\n",
        "#X_test_np = X_test.values.astype('float32')\n",
        "\n",
        "# Para clasificaci√≥n: One-hot encoding del target\n",
        "#if y.dtype == 'object' or y.nunique() < 20:\n",
        "#    num_classes = y.nunique()\n",
        "#    y_train_np = keras.utils.to_categorical(y_train_encoded, num_classes)\n",
        "#    y_val_np = keras.utils.to_categorical(y_val_encoded, num_classes)\n",
        "#    y_test_np = keras.utils.to_categorical(y_test_encoded, num_classes)\n",
        "#else:\n",
        "#    y_train_np = y_train.values.astype('float32')\n",
        "#    y_val_np = y_val.values.astype('float32')\n",
        "#    y_test_np = y_test.values.astype('float32')\n",
        "\n",
        "#print(f\"\\n‚úÖ Datos preparados para TensorFlow/Keras\")\n",
        "#print(f\"   Shape X_train: {X_train_np.shape}\")\n",
        "#print(f\"   Shape y_train: {y_train_np.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOAQ5Pc9WyQs"
      },
      "source": [
        "---\n",
        "## 6. Dise√±o y Arquitectura del Modelo\n",
        "\n",
        "### 6.1 Justificaci√≥n de la Arquitectura\n",
        "\n",
        "**Instrucciones:** Justifique la elecci√≥n de su arquitectura de red neuronal:\n",
        "- ¬øPor qu√© eligi√≥ este tipo de arquitectura?\n",
        "- ¬øQu√© alternativas consider√≥?\n",
        "- ¬øC√≥mo determin√≥ el n√∫mero de capas y neuronas?\n",
        "\n",
        "---\n",
        "\n",
        "*[Se seleccion√≥ una arquitectura LSTM debido a que la degradaci√≥n de componentes mec√°nicos se manifiesta como patrones temporales (tendencias y cambios de variabiliad) en las se√±ales de vibraci√≥n. La LSTM permite modelar explicitamente la dependencia temporal mediante su estado de memoria, capturando se√±ales precursoras de falla con menor dependencia de ingenieria manual de variables temporales.\n",
        "Se consideraron alternativas como modelos tabulares (LightGBM/XGBoost), CNN/TCN y Transformers.\n",
        "LightGBM ofrece alta interpretabilidad y desempe√±o en desbalance, pero no modela la secuencia completa sin feature engineering; CNN/TCN son competitivas y se proponen como evoluci√≥n del sistema; Transformers se descartaron en esta fase por requerimientos de datos y complejidad\n",
        "El n√∫mero de capas y neuronas se determin√≥ mediante una b√∫squeda controlada (capas{1,2} y neuronas {32,64,128}), seleccionanso la configuraci√≥n con mejor PR-AUC en validaci√≥n temporal y cumpliendo restricciones operacionales. Se aplic√≥ early stopping y dropout para evitar sobreajuste dad la baja frecuencia de fallas]*\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 6.2 Definici√≥n del Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VC1ibd1WyQs"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# DEFINICI√ìN DEL MODELO CON PYTORCH\n",
        "# =====================================================\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Red Neuronal para [Clasificaci√≥n/Regresi√≥n]\n",
        "\n",
        "    Arquitectura:\n",
        "    - Capa de entrada: [n_features] neuronas\n",
        "    - Capas ocultas: [Describir]\n",
        "    - Capa de salida: [n_outputs] neuronas\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate=0.3):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        prev_size = input_size\n",
        "\n",
        "        # Capas ocultas\n",
        "        for hidden_size in hidden_sizes:\n",
        "            layers.append(nn.Linear(prev_size, hidden_size))\n",
        "            layers.append(nn.BatchNorm1d(hidden_size))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(dropout_rate))\n",
        "            prev_size = hidden_size\n",
        "\n",
        "        # Capa de salida\n",
        "        layers.append(nn.Linear(prev_size, output_size))\n",
        "\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "# =====================================================\n",
        "# CONFIGURACI√ìN DEL MODELO\n",
        "# =====================================================\n",
        "\n",
        "INPUT_SIZE = X_train.shape[1]\n",
        "HIDDEN_SIZES = [128, 64, 32]  # Ajuste seg√∫n su problema\n",
        "OUTPUT_SIZE = y.nunique() if (y.dtype == 'object' or y.nunique() < 20) else 1\n",
        "DROPOUT_RATE = 0.3\n",
        "\n",
        "# Crear modelo\n",
        "model_pytorch = NeuralNetwork(INPUT_SIZE, HIDDEN_SIZES, OUTPUT_SIZE, DROPOUT_RATE)\n",
        "model_pytorch = model_pytorch.to(device)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ARQUITECTURA DEL MODELO (PyTorch)\")\n",
        "print(\"=\" * 60)\n",
        "print(model_pytorch)\n",
        "\n",
        "# Contar par√°metros\n",
        "total_params = sum(p.numel() for p in model_pytorch.parameters())\n",
        "trainable_params = sum(p.numel() for p in model_pytorch.parameters() if p.requires_grad)\n",
        "print(f\"\\nüìä Par√°metros totales: {total_params:,}\")\n",
        "print(f\"   Par√°metros entrenables: {trainable_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8gYzDD5WyQs"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# DEFINICI√ìN DEL MODELO CON KERAS (ALTERNATIVA)\n",
        "# =====================================================\n",
        "\n",
        "#def create_keras_model(input_shape, hidden_sizes, output_size, dropout_rate=0.3, task='classification'):\n",
        "#    \"\"\"\n",
        "#    Crea un modelo de red neuronal con Keras.\n",
        "\n",
        "#    Args:\n",
        "#        input_shape: Dimensi√≥n de entrada\n",
        "#        hidden_sizes: Lista con el n√∫mero de neuronas por capa oculta\n",
        "#        output_size: N√∫mero de neuronas de salida\n",
        "#        dropout_rate: Tasa de dropout\n",
        "#        task: 'classification' o 'regression'\n",
        "#    \"\"\"\n",
        "#    model = keras.Sequential()\n",
        "\n",
        "#    # Capa de entrada\n",
        "#    model.add(layers.Input(shape=(input_shape,)))\n",
        "\n",
        "#    # Capas ocultas\n",
        "#    for hidden_size in hidden_sizes:\n",
        "#        model.add(layers.Dense(hidden_size))\n",
        "#        model.add(layers.BatchNormalization())\n",
        "#        model.add(layers.Activation('relu'))\n",
        "#        model.add(layers.Dropout(dropout_rate))\n",
        "\n",
        "    # Capa de salida\n",
        "#    if task == 'classification':\n",
        "#        if output_size == 2:\n",
        "#            model.add(layers.Dense(1, activation='sigmoid'))\n",
        "#        else:\n",
        "#            model.add(layers.Dense(output_size, activation='softmax'))\n",
        "#    else:\n",
        "#        model.add(layers.Dense(1, activation='linear'))\n",
        "\n",
        "#    return model\n",
        "\n",
        "# Crear modelo Keras\n",
        "#TASK = 'classification'  # Cambie a 'regression' si es necesario\n",
        "\n",
        "#model_keras = create_keras_model(\n",
        "#    input_shape=INPUT_SIZE,\n",
        "#    hidden_sizes=HIDDEN_SIZES,\n",
        "#    output_size=OUTPUT_SIZE,\n",
        "#    dropout_rate=DROPOUT_RATE,\n",
        "#    task=TASK\n",
        "#)\n",
        "\n",
        "#print(\"=\" * 60)\n",
        "#print(\"ARQUITECTURA DEL MODELO (Keras)\")\n",
        "#print(\"=\" * 60)\n",
        "#model_keras.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riURT89XWyQx"
      },
      "source": [
        "### 6.3 Diagrama de la Arquitectura\n",
        "\n",
        "**Instrucciones:** Incluya un diagrama visual de su arquitectura de red neuronal.\n",
        "\n",
        "---\n",
        "\n",
        "*[Inserte diagrama o descripci√≥n visual de la arquitectura]*\n",
        "\n",
        "```\n",
        "Input Layer          Hidden Layer 1       Hidden Layer 2       Output Layer\n",
        "[n features]   -->   [128 neurons]   -->  [64 neurons]    -->  [n classes]\n",
        "                     + BatchNorm          + BatchNorm\n",
        "                     + ReLU               + ReLU\n",
        "                     + Dropout(0.3)       + Dropout(0.3)\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3SDwbBjWyQx"
      },
      "source": [
        "---\n",
        "## 7. Entrenamiento del Modelo\n",
        "\n",
        "### 7.1 Configuraci√≥n del Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlceVmozWyQx"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# HIPERPAR√ÅMETROS DE ENTRENAMIENTO\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CONFIGURACI√ìN DEL ENTRENAMIENTO\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Hiperpar√°metros\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 32\n",
        "EARLY_STOPPING_PATIENCE = 10\n",
        "\n",
        "print(f\"\\nüìã Hiperpar√°metros:\")\n",
        "print(f\"   Learning Rate: {LEARNING_RATE}\")\n",
        "print(f\"   Epochs: {EPOCHS}\")\n",
        "print(f\"   Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"   Early Stopping Patience: {EARLY_STOPPING_PATIENCE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VH6Gpa6FWyQx"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# CONFIGURACI√ìN DE LOSS Y OPTIMIZADOR (PyTorch)\n",
        "# =====================================================\n",
        "\n",
        "# Seleccionar funci√≥n de p√©rdida seg√∫n el tipo de problema\n",
        "if y.dtype == 'object' or y.nunique() < 20:\n",
        "    # Clasificaci√≥n\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    task_type = 'classification'\n",
        "else:\n",
        "    # Regresi√≥n\n",
        "    criterion = nn.MSELoss()\n",
        "    task_type = 'regression'\n",
        "\n",
        "# Optimizador\n",
        "optimizer = optim.Adam(model_pytorch.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        ")\n",
        "\n",
        "print(f\"\\nüìã Configuraci√≥n:\")\n",
        "print(f\"   Tipo de problema: {task_type}\")\n",
        "print(f\"   Funci√≥n de p√©rdida: {criterion}\")\n",
        "print(f\"   Optimizador: Adam\")\n",
        "print(f\"   Scheduler: ReduceLROnPlateau\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MhbREX6WyQx"
      },
      "source": [
        "### 7.2 Entrenamiento del Modelo (PyTorch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1w9MQ022WyQx"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# FUNCIONES DE ENTRENAMIENTO Y EVALUACI√ìN\n",
        "# =====================================================\n",
        "\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"Entrena el modelo por una √©poca.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if task_type == 'classification':\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = correct / total if task_type == 'classification' else None\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def evaluate(model, val_loader, criterion, device):\n",
        "    \"\"\"Eval√∫a el modelo en el conjunto de validaci√≥n.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if task_type == 'classification':\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += y_batch.size(0)\n",
        "                correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(val_loader)\n",
        "    accuracy = correct / total if task_type == 'classification' else None\n",
        "\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_T7BDPqWyQx"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# ENTRENAMIENTO DEL MODELO (PyTorch)\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ENTRENAMIENTO DEL MODELO\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Historial de entrenamiento\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'val_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_acc': []\n",
        "}\n",
        "\n",
        "# Early stopping\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "best_model_state = None\n",
        "\n",
        "print(f\"\\nüöÄ Iniciando entrenamiento...\\n\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # Entrenamiento\n",
        "    train_loss, train_acc = train_epoch(model_pytorch, train_loader, criterion, optimizer, device)\n",
        "\n",
        "    # Validaci√≥n\n",
        "    val_loss, val_acc = evaluate(model_pytorch, val_loader, criterion, device)\n",
        "\n",
        "    # Guardar historial\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    if task_type == 'classification':\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "    # Scheduler step\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Imprimir progreso cada 10 √©pocas\n",
        "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "        if task_type == 'classification':\n",
        "            print(f\"√âpoca {epoch+1:3d}/{EPOCHS} | \"\n",
        "                  f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
        "                  f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "        else:\n",
        "            print(f\"√âpoca {epoch+1:3d}/{EPOCHS} | \"\n",
        "                  f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        best_model_state = model_pytorch.state_dict().copy()\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
        "            print(f\"\\n‚ö†Ô∏è Early stopping en √©poca {epoch+1}\")\n",
        "            break\n",
        "\n",
        "# Cargar mejor modelo\n",
        "if best_model_state is not None:\n",
        "    model_pytorch.load_state_dict(best_model_state)\n",
        "    print(f\"\\n‚úÖ Mejor modelo cargado (Val Loss: {best_val_loss:.4f})\")\n",
        "\n",
        "print(f\"\\nüéâ Entrenamiento completado!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESniQf2JWyQx"
      },
      "source": [
        "### 7.3 Entrenamiento del Modelo (Keras - Alternativa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_Am-VoCWyQx"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# ENTRENAMIENTO DEL MODELO (KERAS)\n",
        "# =====================================================\n",
        "\n",
        "# Compilar modelo\n",
        "#if TASK == 'classification':\n",
        "#    if OUTPUT_SIZE == 2:\n",
        "#        model_keras.compile(\n",
        "#            optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "#            loss='binary_crossentropy',\n",
        "#            metrics=['accuracy']\n",
        "#        )\n",
        "#    else:\n",
        "#        model_keras.compile(\n",
        "#            optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "#            loss='categorical_crossentropy',\n",
        "#            metrics=['accuracy']\n",
        "#        )\n",
        "#else:\n",
        "#    model_keras.compile(\n",
        "#        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "#        loss='mse',\n",
        "#        metrics=['mae']\n",
        "#    )\n",
        "#\n",
        "# Callbacks\n",
        "#keras_callbacks = [\n",
        "#    callbacks.EarlyStopping(\n",
        "#        monitor='val_loss',\n",
        "#        patience=EARLY_STOPPING_PATIENCE,\n",
        "#        restore_best_weights=True,\n",
        "#        verbose=1\n",
        "#    ),\n",
        "#    callbacks.ReduceLROnPlateau(\n",
        "#        monitor='val_loss',\n",
        "#        factor=0.5,\n",
        "#        patience=5,\n",
        "#        verbose=1\n",
        "#    ),\n",
        "#    callbacks.ModelCheckpoint(\n",
        "#        'best_model.keras',\n",
        "#        monitor='val_loss',\n",
        "#        save_best_only=True,\n",
        "#        verbose=0\n",
        "#    )\n",
        "#]\n",
        "\n",
        "# Entrenar\n",
        "#print(\"=\" * 60)\n",
        "#print(\"ENTRENAMIENTO DEL MODELO (KERAS)\")\n",
        "#print(\"=\" * 60)\n",
        "\n",
        "#history_keras = model_keras.fit(\n",
        "#    X_train_np, y_train_np,\n",
        "#    validation_data=(X_val_np, y_val_np),\n",
        "#    epochs=EPOCHS,\n",
        "#    batch_size=BATCH_SIZE,\n",
        "#    callbacks=keras_callbacks,\n",
        "#    verbose=1\n",
        "#)\n",
        "\n",
        "#print(\"\\nüéâ Entrenamiento completado!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eI3WoQ8CWyQx"
      },
      "source": [
        "### 7.4 Visualizaci√≥n del Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TLZnTD4WyQx"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# VISUALIZACI√ìN DEL PROCESO DE ENTRENAMIENTO\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CURVAS DE APRENDIZAJE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Gr√°fico de p√©rdida\n",
        "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
        "axes[0].plot(history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "axes[0].set_title('Evoluci√≥n de la P√©rdida', fontsize=14)\n",
        "axes[0].set_xlabel('√âpoca')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Gr√°fico de precisi√≥n (solo para clasificaci√≥n)\n",
        "if task_type == 'classification':\n",
        "    axes[1].plot(history['train_acc'], label='Train Accuracy', linewidth=2)\n",
        "    axes[1].plot(history['val_acc'], label='Validation Accuracy', linewidth=2)\n",
        "    axes[1].set_title('Evoluci√≥n de la Precisi√≥n', fontsize=14)\n",
        "    axes[1].set_xlabel('√âpoca')\n",
        "    axes[1].set_ylabel('Accuracy')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "else:\n",
        "    axes[1].text(0.5, 0.5, 'N/A para Regresi√≥n', ha='center', va='center', fontsize=14)\n",
        "    axes[1].set_title('Precisi√≥n (No aplica)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# An√°lisis del entrenamiento\n",
        "print(\"\\nüìä An√°lisis del Entrenamiento:\")\n",
        "print(f\"   √âpocas completadas: {len(history['train_loss'])}\")\n",
        "print(f\"   Mejor val_loss: {min(history['val_loss']):.4f} (√©poca {history['val_loss'].index(min(history['val_loss']))+1})\")\n",
        "if task_type == 'classification':\n",
        "    print(f\"   Mejor val_acc: {max(history['val_acc']):.4f} (√©poca {history['val_acc'].index(max(history['val_acc']))+1})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRJxkKpUWyQy"
      },
      "source": [
        "---\n",
        "## 8. Evaluaci√≥n y M√©tricas\n",
        "\n",
        "### 8.1 Evaluaci√≥n en el Conjunto de Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlHbAvp_WyQy"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# EVALUACI√ìN EN EL CONJUNTO DE TEST\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"EVALUACI√ìN EN CONJUNTO DE TEST\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Hacer predicciones\n",
        "model_pytorch.eval()\n",
        "with torch.no_grad():\n",
        "    X_test_device = X_test_tensor.to(device)\n",
        "    outputs = model_pytorch(X_test_device)\n",
        "\n",
        "    if task_type == 'classification':\n",
        "        _, y_pred = torch.max(outputs, 1)\n",
        "        y_pred = y_pred.cpu().numpy()\n",
        "        y_true = y_test_tensor.numpy()\n",
        "        y_proba = torch.softmax(outputs, dim=1).cpu().numpy()\n",
        "    else:\n",
        "        y_pred = outputs.cpu().numpy().flatten()\n",
        "        y_true = y_test_tensor.numpy().flatten()\n",
        "\n",
        "print(f\"\\n‚úÖ Predicciones realizadas: {len(y_pred)} muestras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJW0jGXNWyQy"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# M√âTRICAS DE CLASIFICACI√ìN\n",
        "# =====================================================\n",
        "\n",
        "if task_type == 'classification':\n",
        "    print(\"=\" * 60)\n",
        "    print(\"M√âTRICAS DE CLASIFICACI√ìN\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Calcular m√©tricas\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='weighted')\n",
        "    recall = recall_score(y_true, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "    print(f\"\\nüìä M√©tricas Principales:\")\n",
        "    print(f\"   Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"   Precision: {precision:.4f}\")\n",
        "    print(f\"   Recall:    {recall:.4f}\")\n",
        "    print(f\"   F1-Score:  {f1:.4f}\")\n",
        "\n",
        "    # Reporte de clasificaci√≥n completo\n",
        "    print(f\"\\nüìã Reporte de Clasificaci√≥n Detallado:\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "    # Matriz de confusi√≥n\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=range(OUTPUT_SIZE),\n",
        "                yticklabels=range(OUTPUT_SIZE))\n",
        "    plt.title('Matriz de Confusi√≥n', fontsize=14)\n",
        "    plt.xlabel('Predicci√≥n')\n",
        "    plt.ylabel('Valor Real')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BY9H5YqFWyQy"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# M√âTRICAS DE REGRESI√ìN\n",
        "# =====================================================\n",
        "\n",
        "if task_type == 'regression':\n",
        "    print(\"=\" * 60)\n",
        "    print(\"M√âTRICAS DE REGRESI√ìN\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Calcular m√©tricas\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"\\nüìä M√©tricas de Regresi√≥n:\")\n",
        "    print(f\"   MSE:  {mse:.4f}\")\n",
        "    print(f\"   RMSE: {rmse:.4f}\")\n",
        "    print(f\"   MAE:  {mae:.4f}\")\n",
        "    print(f\"   R¬≤:   {r2:.4f}\")\n",
        "\n",
        "    # Gr√°fico de predicciones vs valores reales\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Scatter plot\n",
        "    axes[0].scatter(y_true, y_pred, alpha=0.5)\n",
        "    axes[0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
        "    axes[0].set_xlabel('Valor Real')\n",
        "    axes[0].set_ylabel('Predicci√≥n')\n",
        "    axes[0].set_title('Predicciones vs Valores Reales')\n",
        "\n",
        "    # Distribuci√≥n de residuos\n",
        "    residuos = y_true - y_pred\n",
        "    axes[1].hist(residuos, bins=50, edgecolor='black')\n",
        "    axes[1].axvline(x=0, color='r', linestyle='--')\n",
        "    axes[1].set_xlabel('Residuo')\n",
        "    axes[1].set_ylabel('Frecuencia')\n",
        "    axes[1].set_title('Distribuci√≥n de Residuos')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD8d8HCMWyQy"
      },
      "source": [
        "### 8.2 Comparaci√≥n con Modelo Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-MAvpglWyQy"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# COMPARACI√ìN CON MODELO BASELINE\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"COMPARACI√ìN CON MODELO BASELINE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if task_type == 'classification':\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "    # Modelos baseline\n",
        "    baselines = {\n",
        "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=RANDOM_SEED),\n",
        "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED)\n",
        "    }\n",
        "else:\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "\n",
        "    baselines = {\n",
        "        'Linear Regression': LinearRegression(),\n",
        "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=RANDOM_SEED)\n",
        "    }\n",
        "\n",
        "# Entrenar y evaluar baselines\n",
        "results = {'Modelo': [], 'M√©trica': []}\n",
        "\n",
        "for name, model in baselines.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred_baseline = model.predict(X_test)\n",
        "\n",
        "    if task_type == 'classification':\n",
        "        metric = accuracy_score(y_test, y_pred_baseline)\n",
        "        metric_name = 'Accuracy'\n",
        "    else:\n",
        "        metric = r2_score(y_test, y_pred_baseline)\n",
        "        metric_name = 'R¬≤'\n",
        "\n",
        "    results['Modelo'].append(name)\n",
        "    results['M√©trica'].append(metric)\n",
        "\n",
        "# Agregar modelo de Deep Learning\n",
        "results['Modelo'].append('Deep Learning')\n",
        "if task_type == 'classification':\n",
        "    results['M√©trica'].append(accuracy)\n",
        "else:\n",
        "    results['M√©trica'].append(r2)\n",
        "\n",
        "# Mostrar comparaci√≥n\n",
        "comparison_df = pd.DataFrame(results)\n",
        "comparison_df = comparison_df.sort_values('M√©trica', ascending=False)\n",
        "\n",
        "print(f\"\\nüìä Comparaci√≥n de Modelos ({metric_name}):\")\n",
        "display(comparison_df)\n",
        "\n",
        "# Visualizaci√≥n\n",
        "plt.figure(figsize=(10, 6))\n",
        "colors = ['#2ecc71' if m == 'Deep Learning' else '#3498db' for m in comparison_df['Modelo']]\n",
        "plt.barh(comparison_df['Modelo'], comparison_df['M√©trica'], color=colors)\n",
        "plt.xlabel(metric_name)\n",
        "plt.title(f'Comparaci√≥n de Modelos - {metric_name}')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdrSJo8MWyQy"
      },
      "source": [
        "### 8.3 An√°lisis de Resultados\n",
        "\n",
        "**Instrucciones:** Analice los resultados obtenidos:\n",
        "\n",
        "---\n",
        "\n",
        "**Rendimiento del Modelo:**\n",
        "*[Analice las m√©tricas obtenidas]*\n",
        "\n",
        "**Comparaci√≥n con Baselines:**\n",
        "*[Compare el rendimiento con los modelos baseline]*\n",
        "\n",
        "**Fortalezas del Modelo:**\n",
        "1. *[Capacidad para capturar patrones previos a la falla]*\n",
        "2. *[Mejores me√©tricas en escenarios desbalanceados]*\n",
        "3. *[Potencial para entregar valor operacional]*\n",
        "\n",
        "**Debilidades del Modelo:**\n",
        "1. *[Sensibilidad a cambios operacionales y falsas alarmas]*\n",
        "2. *[Dependencia cr√≠tica del etiquetado por casos de eventos mal registrados]*\n",
        "\n",
        "**Posibles Mejoras:**\n",
        "1. *[Filtrar transitorios excluyendo arranques/paradas]*\n",
        "2. *[Mejorar las etiquetas y definici√≥n del evento ocurrido]*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUiUxDoeWyQy"
      },
      "source": [
        "---\n",
        "## 9. Interpretaci√≥n de Resultados\n",
        "\n",
        "### 9.1 Importancia de Features (SHAP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JZhMbqYWyQy"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# INTERPRETABILIDAD CON SHAP (OPCIONAL)\n",
        "# =====================================================\n",
        "\n",
        "# Instalar SHAP si no est√° disponible\n",
        "# !pip install shap\n",
        "\n",
        "#try:\n",
        "#    import shap\n",
        "\n",
        "#    print(\"=\" * 60)\n",
        "#    print(\"AN√ÅLISIS DE IMPORTANCIA DE FEATURES (SHAP)\")\n",
        "#    print(\"=\" * 60)\n",
        "\n",
        "#    # Crear explainer\n",
        "#    # Usar una muestra del dataset para acelerar el c√°lculo\n",
        "#    sample_size = min(100, len(X_test))\n",
        "#    X_sample = X_test.iloc[:sample_size]\n",
        "\n",
        "#    # Para modelos de sklearn (baselines)\n",
        "#    explainer = shap.TreeExplainer(baselines['Random Forest'])\n",
        "#    shap_values = explainer.shap_values(X_sample)\n",
        "\n",
        "    # Visualizaci√≥n\n",
        "#    plt.figure(figsize=(12, 8))\n",
        "#    if task_type == 'classification' and len(shap_values) > 1:\n",
        "#        shap.summary_plot(shap_values[1], X_sample, plot_type=\"bar\", show=False)\n",
        "#    else:\n",
        "#        shap.summary_plot(shap_values, X_sample, plot_type=\"bar\", show=False)\n",
        "#    plt.title('Importancia de Features (SHAP)')\n",
        "#    plt.tight_layout()\n",
        "#    plt.show()\n",
        "\n",
        "#except ImportError:\n",
        "#    print(\"‚ö†Ô∏è SHAP no est√° instalado. Ejecute: !pip install shap\")\n",
        "#except Exception as e:\n",
        "#    print(f\"‚ö†Ô∏è Error en an√°lisis SHAP: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhV7uWO3WyQy"
      },
      "source": [
        "### 9.2 Interpretaci√≥n de Negocios\n",
        "\n",
        "**Instrucciones:** Traduzca los resultados t√©cnicos a insights de negocio:\n",
        "\n",
        "---\n",
        "\n",
        "**Insights Principales:**\n",
        "1. *[Insight 1 - ¬øQu√© significa el resultado para el negocio?]*\n",
        "2. *[Insight 2]*\n",
        "3. *[Insight 3]*\n",
        "\n",
        "**Factores M√°s Importantes:**\n",
        "*[¬øCu√°les son los factores m√°s importantes seg√∫n el modelo y qu√© significan para el negocio?]*\n",
        "\n",
        "**Patrones Identificados:**\n",
        "*[¬øQu√© patrones ha identificado el modelo que pueden ser relevantes para la toma de decisiones?]*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4cFxRMtWyQy"
      },
      "source": [
        "---\n",
        "## 10. Conclusiones y Recomendaciones de Negocio\n",
        "\n",
        "### 10.1 Resumen de Resultados\n",
        "\n",
        "**Instrucciones:** Proporcione un resumen ejecutivo de los resultados:\n",
        "\n",
        "---\n",
        "\n",
        "*[Desarrollo y evaluaci√≥n de un modelo de ML/DL orietado a anticipar fallas en motores el√©ctricos de MT en una planta minera, utilizando informaci√≥n preveniente de sensores de vibraci√≥n y teperatura. El objetivo operacional es detectar condiciones incipientes con suficiente apnticipaci√≥n para planificar inspecciones, reouestos y paradas programadas, reduciendo eventos no planificados y costos asociados]*\n",
        "\n",
        "---\n",
        "\n",
        "### 10.2 Conclusiones\n",
        "\n",
        "**Instrucciones:** Liste las conclusiones principales:\n",
        "\n",
        "---\n",
        "\n",
        "1. *[El modelo demuestra capacidad para distinguir condiciones normales vs condiciones pre-falla en un entorno naturalmente desbalanceado.]*\n",
        "2. *[Al seleccionar un umbral operativo, el sistema logra generar alertas accionables, maximizando la detecci√≥n de eventos y controlando la carga de trabjo por falsas alarmas]*\n",
        "3. *[El an√°lisis de lead time evidencia que, para una parte relevante de los eventos, elmodelo puede entregar anticipaci√≥n √∫til antes de la intervenci√≥n, habiliatando una gesti√≥n m√°s proactiva]*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVPW2JU1WyQy"
      },
      "source": [
        "---\n",
        "## 11. Referencias\n",
        "\n",
        "**Instrucciones:** Liste todas las referencias utilizadas (formato APA):\n",
        "\n",
        "---\n",
        "\n",
        "1. *[Referencia 1]*\n",
        "2. *[Referencia 2]*\n",
        "3. *[Referencia 3]*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXVT8J5wWyQy"
      },
      "source": [
        "---\n",
        "## Anexos\n",
        "\n",
        "### A. Guardado del Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_S38qS24WyQy"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# GUARDAR EL MODELO ENTRENADO\n",
        "# =====================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"GUARDADO DEL MODELO\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Guardar modelo PyTorch\n",
        "MODEL_PATH = 'modelo_final.pth'\n",
        "torch.save({\n",
        "    'model_state_dict': model_pytorch.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'history': history,\n",
        "    'hyperparameters': {\n",
        "        'input_size': INPUT_SIZE,\n",
        "        'hidden_sizes': HIDDEN_SIZES,\n",
        "        'output_size': OUTPUT_SIZE,\n",
        "        'dropout_rate': DROPOUT_RATE,\n",
        "        'learning_rate': LEARNING_RATE\n",
        "    }\n",
        "}, MODEL_PATH)\n",
        "\n",
        "print(f\"\\n‚úÖ Modelo PyTorch guardado en: {MODEL_PATH}\")\n",
        "\n",
        "# Guardar modelo Keras (opcional)\n",
        "# model_keras.save('modelo_final.keras')\n",
        "# print(f\"‚úÖ Modelo Keras guardado en: modelo_final.keras\")\n",
        "\n",
        "# Guardar scaler\n",
        "import joblib\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "print(f\"‚úÖ Scaler guardado en: scaler.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooZOKEZUWyQz"
      },
      "source": [
        "---\n",
        "\n",
        "## Checklist de Entrega\n",
        "\n",
        "Antes de entregar, verifique que ha completado los siguientes elementos:\n",
        "\n",
        "- [ ] Informaci√≥n del proyecto completada\n",
        "- [ ] Resumen ejecutivo escrito\n",
        "- [ ] Problema de negocio claramente definido\n",
        "- [ ] Objetivos SMART establecidos\n",
        "- [ ] EDA completo con visualizaciones\n",
        "- [ ] Preprocesamiento de datos documentado\n",
        "- [ ] Arquitectura del modelo justificada\n",
        "- [ ] Modelo entrenado con curvas de aprendizaje\n",
        "- [ ] M√©tricas de evaluaci√≥n calculadas\n",
        "- [ ] Comparaci√≥n con modelos baseline\n",
        "- [ ] Interpretaci√≥n de resultados\n",
        "- [ ] Conclusiones y recomendaciones de negocio\n",
        "- [ ] Referencias listadas\n",
        "- [ ] C√≥digo ejecutable sin errores\n",
        "- [ ] Comentarios y documentaci√≥n adecuados\n",
        "\n",
        "---\n",
        "\n",
        "**¬°Buena suerte con su proyecto!** üéì"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}